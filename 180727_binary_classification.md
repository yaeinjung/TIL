# binary_classification

random forest

앞의 70%를 학습하고 앞에 30%를 예측하는 것임

train data가 많아야 학습이 잘되서 70%를 학습시키는것!

* df의 split_count까지의데이터를 가져옴

```
df_train = df[:split_count]
```

### 단어벡터화

max_feature는 너무 많이 만들면 오류가 날 수 있음. 많을수록 좋지만 시간은 느려짐



만표 이상을 아웃라이어로 잡음 -> 95%

1. 아웃라이어 데이터제거하기 -> describe를 봄 표준편차가 큼. 특정값에 몰려있다는 것을 알았음

   투표수 100 이상인것과 1000 이하를 뽑음 -> 데이터가 많아야 학습이 잘되니까 데이터의 모수를 늘렸음.

2. 아웃라이어 데이터는 최대한 제거를 했음 다 똑같이 만들어주고 랜덤포레스트에서 n_estimator를 크게 잡음. 

3. 랜덤포레스트에서 학습시킬때 n_estimator가 가장 중요함 (100->200)

4. 투표수가 평균보다 높은지 낮은지만 예측해서 label에 투표수만 넣었는데 카테고리를 예측하고 싶으면 레이블에 카테고리를 넣으면 된다!

5. 랜덤포레스트말고 딥러닝 라이브러리를 쓰면 훨씬 더 잘됨 tensorflow같은