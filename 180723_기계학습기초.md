# 기계학습기초



* 분류, 클러스터링, 회귀분석, 차원축소
* 대표적인 딥러닝 예제 
  * 분류 : 개와 고양이 분류, 웹사이트에서 로봇인지 사람인지 구분
  * 회귀 : 수요예측(얼마나 물량이 필요할지, 배송기사들이 몇명이 필요할지)
  * 클러스터링 : 군집화, 
  * 차원축소 :  단어의 유사도를 측정, 그 유사도를 차원축소해서 보여줌

* Sickit Learn
  * 파이썬의 대표적인 기계학습 프로그램



### 지난시간 실습에서 불용어 제거하기

- 단어중 `은,는,이,가`가 포함된 단어가 있을 수도 있다. ex. 화**이**팅 
- -> 그래서 토큰화 한다음에 토큰에서 빼는 것이 좋다.

```
def remove_stopwords(text):
    stops = ['수', '있는', '있습니다', '그', '년도', '에', '합니다', '하는', '및', '제', '할', '하고', '더', '대한', '한', '그리고', '월', '저는', '없는', '것입니다', '등', '일', '많은', '이런', '것은', '왜', '같은', '없습니다', '위해', '한다']
    # Stopwords 불용어 제거
    meaningful_words = [word for word in text if not word in stops]
    return ' '.join(meaningful_words)
```

* 공백으로 띄워주는 이유는 단어끼리 붙기때문

```
def preprocessing(text):
    # 개행문자 제거
    text = re.sub('\\\\n', ' ', text) # 공백으로 안하면 뒤에 단어하고 붙어버린다.
    text = re.sub('000원',' ', text)
    return text
```

* `%`  : 한줄에 대한 시간을 재어줌
* `%%`  : 여러줄에 대한 시간을 잼

```
%time tokens = sentences.apply(tokenizer.tokenize)
tokens[:3]
```



* 명사추출 후 토큰으로 만든것에서 스탑워즈를 빼게 함
* re는 정규표현식.  정규표현식을 다루는 책도 있음
* 

## Machine Learning - Scikit Learn

supervised machine learning (지도학습)

* label이 있다

* 결측치를 제거해야지 일반화된 것에 예측할수있다.

* 분류와 회귀는 거의 지도학습이다.

* 선언

  ```
  clf = RandomForestClassifier()
  ```

* 학습

  ```
  clf.fit(X_train, y_train)
  ```

* 예측

  ```
  y_pred = clf.predict(X_test)
  ```

* 평가 (잘 예측이 되었나)

  ```
  clf.score(X_test, y_test)
  ```

* x_train 데이터에 '방향','크기' 가 있으면 x_test에도 '방향','크기'가 똑같이 있어야 한다.

* 국민청원에서 어떠한 텍스트를 가진 청원은 투표수가 어느정도 될지 예측할 수도 있다. 

* 많은 데이터를 넣고 돌리면 좀 더 정확하다.





Unsupervised Machine Learning (비지도학습)

* label이 없다



### 단어 벡터화

* Tfidf 원래는 검색엔진에서 많이 사용하는 알고리즘

* 현재는 Bag of word에서 단점을 많이 보완하도록 사용하는 중

* underfitting: 제대로 학습되지 않은 상태

* overfitting: 아웃라이어데이터, 결측치가 영향을 줘서 학습이 필요없는 부분까지 과도하게 된 상태

* Decision Tree의 장점은 시각화하기 좋다. 단점은 정확도가 떨어짐

* Random Forests : Decision Tree의 단점을 보완한것 . Decision Tree를 여러개 그려준다.

  

  #### 벡터의 내적과 외적

  * 벡터의 내적 공식은 두 벡터가 있을때 두 벡터 사이의 각도를 구하는 공식이다. 
  * 벡터의 내적은 결국 라디안 실수 값이 나오지만 외적을 구하는 공식은 그냥 벡터가 하나 더 생깁니다.두 개의 벡터가 있을 기준점에 수직으로 못을 하나 꽂으면 못방향으로 벡터가 하나 생깁니다. 두 벡터에 수직인 벡터 가 하나 더 생기는 셈이지요 v1(x,y,z) v2(x,y,z)가 있을 (0, 0, 0)을 출발점으로 한 위로 우뚝선 벡터 n(x, y, z)가 하나 더 생긴단 말이지요. 두 벡터에 수직인 벡터는 사실 두 개 있습니다. 위, 아래입니다. 보통 시계 방향이냐, 반시케 방향이냐 따라서 한 가지만 뽑아 냅니다.



